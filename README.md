![Open CV Github Frame](https://github.com/TH-Activities/saturday-hack-night-template/assets/90635335/78554b37-32b2-4488-a10c-5c68098d7776)



# Project Name
VIDEO ANOMALY DETECTION ## Team members
1. [PERCY REBEKAH](https://github.com/TH-Activities/saturday-hack-night-template)
2. [S S SETHULEKSHMY](https://github.com/TH-Activities/saturday-hack-night-template)
    [](https://github.com/TH-Activities/saturday-hack-night-template)
 3.[ROHAN ROY](https://github.com/TH-Activities/saturday-hack-night-template)
   4. [PARVATHY MK](https://github.com/TH-Activities/saturday-hack-night-template)

  

## Link to product walkthrough
[link to video](Link Here)
## How it Works ?
1. In the real world every human being is monitored through millions of surveillance camera around us.Thereâ€™s a requirement for credible and effective surveillance. Our aim is to use this surveillance systems to monitor the violent activities happening around us. Real time human activities are monitored and the actions are classified into violence and non- violence. If violence activity is detected, an alert is sent to the administrator.This helped authorities in identifying violent attacks and take the necessary steps immediately . 
The development of several deep learning techniques, brought about by the availability of large data sets and computational resources, has resulted in change in the computer vision community. Several techniques with improved performance for addressing problems such as object detection, recognition, tracking, action recognition, caption generation, etc. have been developed as a result. However, despite the recent developments in deep learning, very few deep learning-based techniques have been proposed to tackle the problem of violence detection from videos. Almost all the existing techniques rely on hand-crafted features for generating visual representations of videos. The advantage of deep learning techniques are direct raw input processing, automatic feature extraction.The data sets of 4000 videos of violence and non-violence videos are given as input and the frames are extracted periodically and the prediction is made. Instead of extracting the result of the last pooling layer that had a feature map of a single frame, a sequence of frames can be fed to the system. This is carried out to classify a segment of the video instead of an individual frame. Then the obtained feature maps are as the input of the MobileNetV2 with bidirectional LSTM network, which is a Convolutional Neural Network for Mobile Vision Applications that uses depth-wise separable convolutions to obtain the final classification of the extracted feature frames. In the end, the video in real-time is classified every four seconds into either violent or nonviolent activity. Then the model is saved for real-time implementation where its input is  obtained from the webcam, then it undergoes activity detection and when violence is detected alert is  generated. 
2. Embed video of project demo
## Libraries used
Library Name - Version
## How to configure
Instructions for setting up project
## How to Run
Instructions for running
